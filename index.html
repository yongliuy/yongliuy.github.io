<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yong Liu</title>

    <meta name="author" content="Yong Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Yong Liu
                </p>
                <p>I am a Ph.D. candidate in the <a href="http://www.aiar.xjtu.edu.cn/">National Key Laboratory of Human-Machine Hybrid Augmented Intelligence</a> at <a href="http://en.xjtu.edu.cn/index.html">Xi'an Jiaotong University</a>, where I am fortunate to be under the supervision of <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Prof. Fei Wang</a>. 
                    Prior to my Ph.D. studies, I obtained the M.S. and B.S. degrees from the <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a>, supervised by <a href="https://www.auto.uestc.edu.cn/info/1157/4349.htm">Prof. Xiaorong Hou</a>. <br>
                     </a>
                </p>
                <p style="text-align:center">
                  <a href="mailto:liuy1996v@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=DT0LPIEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yongliuy/">Github</a>
                </p>
              </td>
              <!-- ######################## Photo -->
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yongliuy.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yongliuy.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



          <!-- ######################## Research -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p> 
                  I'm interested in low-level computer vision, deep learning, generative diffusion models, and image processing. 
                  Now most of my research is about image/video super-resolution and diffusion models.</span>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CVMJ.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">Revitalizing Image Dehazing in the Real World: A High-Quality Dataset and A Customized Method</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Qingji Dong, Chao Zhu, Yu Guo, Fei Wang
                <br>
                <em>CVMJ</em>, 2025
                <br>
                <a href="">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuCVMJ2025.bib">bibtex</a>
                <p></p>
                <p>
                  We propose RealHQ-HAZE, a real-world image dehazing dataset comprising collected hazy and generated haze-free image pairs, along with derived haze variations. 
                  Furthermore, we develop a real-world image dehazing network incorporating a Prior-based Feature Compensation Module (PFCM) and a MixCut Consistent Dehazing (MCCD) strategy. 
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/UltraVSR.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2505.19958">
                  <span class="papertitle">UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Jinshan Pan, Yinchuan Li, Qingji Dong, Chao Zhu, Yu Guo, Fei Wang
                <br>
                <em>ACMMM</em>, 2025
                <br>
                <a href="">paper</a>
                /
                <a href="https://arxiv.org/abs/2505.19958">arXiv</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuACMMM2025.bib">bibtex</a>
                /
                <a href="">poster</a>
                /
                <a href="">code</a> 
                <p></p>
                <p>We propose UltraVSR, a one-step diffusion-based framework for ultra-realistic and temporally coherent video super-resolution. It leverages a degradation-aware restoration schedule (DRS) for fast reconstruction, a recurrent temporal shift (RTS) module for efficient temporal modeling without explicit temporal layers, 
                  spatio-temporal joint distillation (SJD) to enhance temporal coherence while preserving fine details, and a temporally asynchronous inference (TAI) strategy to model long-range dependencies under limited memory. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/PatchScaler.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.17158">
                  <span class="papertitle">PatchScaler: An Efficient Patch-Independent Diffusion Model for Image Super-Resolution</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Hang Dong, Jinshan Pan, Qingji Dong, Kai Chen, Rongxiang Zhang, Lean Fu, Fei Wang.
                <br>
                <em>ICCV</em>, 2025
                <br>
                <a href="">paper</a>
                /
                <a href="https://arxiv.org/abs/2405.17158">arXiv</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuICCV2025.bib">bibtex</a>
                /
                <a href="">poster</a>
                /
                <a href="https://github.com/yongliuy/PatchScaler">code</a> 
                <p></p>
                <p>We propose PatchScaler, an efficient patch-independent diffusion pipeline for single image super-resolution. PatchScaler introduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature patches by quantifying their reconstruction difficulty and establishes shortcut paths with different sampling configurations for each group. 
                  In addition, we propose a texture prompt that provides rich texture conditional information to the diffusion model. 
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/RSDiffSR.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.mdpi.com/2072-4292/17/8/1348">
                  <span class="papertitle">Taming a Diffusion Model to Revitalize Remote Sensing Image Super-Resolution</span>
                </a>
                <br>
                Chao Zhu, <strong> Yong Liu </strong>, Shan Huang, Fei Wang
                <br>
                <em>Remote Sensing</em>, 2025
                <br>
                <a href="https://www.mdpi.com/2072-4292/17/8/1348">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuRS2025.bib">bibtex</a>
                <p></p>
                <p>We introduce RSDiffSR, a conditional diffusion-based framework for remote sensing image super-resolution. It enhances visual quality using a large diffusion model as a generative prior, 
                  bridges domain gaps via low-rank adaptation and multi-stage training, and incorporates an enhanced control mechanism to guide content and edge restoration during diffusion.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/PMDN.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://iccvm.org/2025/papers/s4p1-355-cvmj.pdf">
                  <span class="papertitle">Towards Real-world Image Dehazing: A Tailored Dehazing Method and A High-Quality Dataset</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Qingji Dong, Chao Zhu, Yu Guo, Fei Wang
                <br>
                <em>CVM</em>, 2025
                <br>
                <a href="http://iccvm.org/2025/papers/s4p1-355-cvmj.pdf">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuCVM2025.bib">bibtex</a>
                <p></p>
                <p>We introduce RealHQ-HAZE, a new dataset with 200 collected real-world hazy images, corresponding 200 carefully rendered haze-free images, and additional 1000 varicolored hazy images transferred from the collected images. 
                  Further, we propose a Prior-compensated Multi-stage Dehazing Network (PMDN), which can learn different levels of real-world haze distribution through multi-stage progressive learning.
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/DITN.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612128">
                  <span class="papertitle">Unfolding Once is Enough: A Deployment-Friendly Transformer Unit for Super-Resolution</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Hang Dong, Boyang Liang, Songwei Liu, Qingji Dong, Kai Chen, Fangmin Chen, Lean Fu, Fei Wang
                <br>
                <em>ACMMM</em>, 2023
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612128">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuACMMM2023.bib">bibtex</a>
                /
                <a href="https://github.com/yongliuy/DITN/blob/main/assets/poster.png">poster</a>
                /
                <a href="https://github.com/yongliuy/DITN/">code</a> 
                <p></p>
                <p>We propose a deployment-friendly transformer unit namely UFONE (i.e., UnFolding ONce is Enough) and a Deployment-friendly Inner-patch Transformer Network (DITN) for the SISR task, which can achieve favorable performance with low latency and memory usage on both training and deployment platforms. 
                    Furthermore, to further boost the deployment efficiency, we provide an efficient substitution for layer normalization and propose a fusion optimization strategy for specific operators.
                </p>
              </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/LMFA-Net.jpg' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">
                    <span class="papertitle">Local Multi-scale Feature Aggregation Network for Real-time Image Dehazing</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>Pattern Recognition</em>, 2023
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuPR2023.bib">bibtex</a>
                <p></p>
                <p>We propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. 
                    By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, 
                    LMFA-Net can directly restore the final haze-free image. 
                </p>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CFDN.png' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">
                    <span class="papertitle">Cross-channel Fusion Image Dehazing Network with Feature Attention</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>IEEE 21st International Conference on Communication Technology (ICCT)</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuICCT2021.bib">bibtex</a>
                <p></p>
                <p>We propose a cross-channel fusion image dehazing network with feature attention (CFDN), which directly restores the final clear image from the hazy input. 
                    The network design is motivated by three strategies, namely cross-channel fusion, feature attention mechanism, and local residual learning. 
                    We show that they are effective for image dehazing problem. 
                </p>
                </td>
            </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Welcome to Yong Liu's public academic website.
              </p>
            </td>
          </tr>
        </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
