<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yong Liu</title>

    <meta name="author" content="Yong Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Yong Liu
                </p>
                <p>I am a Ph.D. candidate in the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence at Xi'an Jiaotong University , where I am fortunate to be under the supervision of <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Prof. Fei Wang</a>. 
                    Prior to my doctoral studies, I obtained the M.S. and B.S. degrees from University of Electronic Science and Technology of China. <br>
                     </a>
                </p>
                <!-- <p>
                    My research interests broadly lie in deep learning, low-level computer vision, and diffusion probabilistic models.</a>.
                </p> -->
                <p style="text-align:center">
                  <a href="liuy1996v@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=DT0LPIEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yongliuy/">Github</a>
                </p>
              </td>
              <!-- ######################## Photo -->
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yongliuy.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yongliuy.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



          <!-- ######################## Research -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p> 
                  I'm interested in low-level computer vision, deep learning, generative diffusion models, and image processing. Now most of my research is about single image super-resolution.</span>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/DITN.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612128">
                  <span class="papertitle">Unfolding Once is Enough: A Deployment-Friendly Transformer Unit for Super-Resolution</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Hang Dong, Boyang Liang, Songwei Liu, Qingji Dong, Kai Chen, Fangmin Chen, Lean Fu, Fei Wang
                <br>
                <em>ACMMM</em>, 2023
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612128">paper</a>
                /
                <a href="https://arxiv.org/abs/2308.02794">arXiv</a>
                /
                <a href="data/YongLiuACMMM2023.bib">bibtex</a>
                /
                <a href="https://github.com/yongliuy/DITN/blob/main/assets/poster.png">poster</a>
                /
                <a href="https://github.com/yongliuy/DITN/">github</a> 
                <a href="https://github.com/yongliuy/DITN/"><img alt="gitHub stars" src="https://img.shields.io/github/stars/yongliuy/DITN?style=social"></a>
                <p></p>
                <p>We propose a deployment-friendly transformer unit namely UFONE (i.e., UnFolding ONce is Enough) and a Deployment-friendly Inner-patch Transformer Network (DITN) for the SISR task, which can achieve favorable performance with low latency and memory usage on both training and deployment platforms. 
                    Furthermore, to further boost the deployment efficiency, we provide an efficient substitution for layer normalization and propose a fusion optimization strategy for specific operators. 
                </p>
              </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/LMFA-Net.jpg' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">
                    <span class="papertitle">Local Multi-scale Feature Aggregation Network for Real-time Image Dehazing</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>Pattern Recognition</em>, 2023
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">paper</a>
                /
                <a href="data/YongLiuPR2023.bib">bibtex</a>
                <p></p>
                <p>We propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. 
                    By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, 
                    LMFA-Net can directly restore the final haze-free image. 
                </p>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CFDN.png' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">
                    <span class="papertitle">Cross-channel Fusion Image Dehazing Network with Feature Attention</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>IEEE 21st International Conference on Communication Technology (ICCT)</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">paper</a>
                /
                <a href="data/YongLiuICCT2021.bib">bibtex</a>
                <p></p>
                <p>We propose a cross-channel fusion image dehazing network with feature attention (CFDN), which directly restores the final clear image from the hazy input. 
                    The network design is motivated by three strategies, namely cross-channel fusion, feature attention mechanism, and local residual learning. 
                    We show that they are effective for image dehazing problem. 
                </p>
                </td>
            </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Welcome to yongliu's personal website.
              </p>
            </td>
          </tr>
        </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
