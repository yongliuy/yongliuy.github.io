<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yong Liu</title>

    <meta name="author" content="Yong Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Yong Liu
                </p>
                <p>I am a Ph.D. candidate in the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence at Xi'an Jiaotong University, where I am fortunate to be under the supervision of <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Prof. Fei Wang</a>. 
                    Prior to my Ph.D. studies, I obtained the M.S. and B.S. degrees from the University of Electronic Science and Technology of China. <br>
                     </a>
                </p>
                <!-- <p>
                    My research interests broadly lie in deep learning, low-level computer vision, and diffusion probabilistic models.</a>.
                </p> -->
                <p style="text-align:center">
                  <a href="liuy1996v@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=DT0LPIEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yongliuy/">Github</a>
                </p>
              </td>
              <!-- ######################## Photo -->
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yongliuy.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yongliuy.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



          <!-- ######################## Research -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p> 
                  I'm interested in low-level computer vision, deep learning, generative diffusion models, and image processing. 
                  Now most of my research is about single image super-resolution and diffusion models.</span>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/UltraVSR.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2505.19958">
                  <span class="papertitle">UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Jinshan Pan, Yinchuan Li, Qingji Dong, Chao Zhu, Yu Guo, Fei Wang
                <br>
                <em>ACMMM</em>, 2025
                <br>
                <a href="">paper</a>
                /
                <a href="https://arxiv.org/abs/2505.19958">arXiv</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuACMMM2025.bib">bibtex</a>
                /
                <a href="">poster</a>
                /
                <a href="">code</a> 
                <p></p>
                <p>We propose UltraVSR, a one-step diffusion-based framework for ultra-realistic and temporally coherent video super-resolution. It leverages a degradation-aware restoration schedule (DRS) for fast reconstruction, a recurrent temporal shift (RTS) module for efficient temporal modeling without explicit temporal layers, 
                  spatio-temporal joint distillation (SJD) to enhance temporal coherence while preserving fine details, and a temporally asynchronous inference (TAI) strategy to model long-range dependencies under limited memory. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/DITN.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.17158">
                  <span class="papertitle">PatchScaler: An Efficient Patch-Independent Diffusion Model for Image Super-Resolution</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Hang Dong, Jinshan Pan, Qingji Dong, Kai Chen, Rongxiang Zhang, Lean Fu, Fei Wang.
                <br>
                <em>ICCV</em>, 2025
                <br>
                <a href="">paper</a>
                /
                <a href="https://arxiv.org/abs/2405.17158">arXiv</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuICCV2025.bib">bibtex</a>
                /
                <a href="">poster</a>
                /
                <a href="https://github.com/yongliuy/PatchScaler">code</a> 
                <p></p>
                <p>We propose PatchScaler, an efficient patch-independent diffusion pipeline for single image super-resolution. PatchScaler introduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature patches by quantifying their reconstruction difficulty and establishes shortcut paths with different sampling configurations for each group. 
                  In addition, we propose a texture prompt that provides rich texture conditional information to the diffusion model. 
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/RSDiffSR.png' style="width: 160px; height: 160px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.mdpi.com/2072-4292/17/8/1348">
                  <span class="papertitle">Taming a Diffusion Model to Revitalize Remote Sensing Image Super-Resolution</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Hang Dong, Boyang Liang, Songwei Liu, Qingji Dong, Kai Chen, Fangmin Chen, Lean Fu, Fei Wang
                <br>
                <em>ACMMM</em>, 2023
                <br>
                <a href="https://www.mdpi.com/2072-4292/17/8/1348">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuRS2025.bib">bibtex</a>
                <p></p>
                <p>We introduce RSDiffSR, a conditional diffusion-based framework for remote sensing image super-resolution. It enhances visual quality using a large diffusion model as a generative prior, 
                  bridges domain gaps via low-rank adaptation and multi-stage training, and incorporates an enhanced control mechanism to guide content and edge restoration during diffusion.
                </p>
              </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/LMFA-Net.jpg' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">
                    <span class="papertitle">Local Multi-scale Feature Aggregation Network for Real-time Image Dehazing</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>Pattern Recognition</em>, 2023
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032300300X">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuPR2023.bib">bibtex</a>
                <p></p>
                <p>We propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. 
                    By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, 
                    LMFA-Net can directly restore the final haze-free image. 
                </p>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CFDN.png' style="width: 160px; height: 160px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">
                    <span class="papertitle">Cross-channel Fusion Image Dehazing Network with Feature Attention</span>
                </a>
                <br>
                <strong> Yong Liu </strong>, Xiaorong Hou
                <br>
                <em>IEEE 21st International Conference on Communication Technology (ICCT)</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9657906">paper</a>
                /
                <a href="https://github.com/yongliuy/yongliuy.github.io/blob/main/data/YongLiuICCT2021.bib">bibtex</a>
                <p></p>
                <p>We propose a cross-channel fusion image dehazing network with feature attention (CFDN), which directly restores the final clear image from the hazy input. 
                    The network design is motivated by three strategies, namely cross-channel fusion, feature attention mechanism, and local residual learning. 
                    We show that they are effective for image dehazing problem. 
                </p>
                </td>
            </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Welcome to Yong Liu's public academic website.
              </p>
            </td>
          </tr>
        </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
